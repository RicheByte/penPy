import os
import requests
import openai
import argparse
import time
import json
import ssl
import socket
import threading
import logging
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from requests.exceptions import RequestException
from requests.packages.urllib3.util.retry import Retry
from cryptography import x509
from cryptography.hazmat.backends import default_backend

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

# Load OpenAI API key securely from environment variable
openai.api_key = os.getenv('OPENAI_API_KEY')
if not openai.api_key:
    logging.error("Missing OpenAI API key. Please set the OPENAI_API_KEY environment variable.")
    exit(1)

# Thread-safe data structures
SESSION = requests.Session()
REPORT_DATA = {'vulnerabilities': [], 'timestamp': time.time()}
report_lock = threading.Lock()
rate_limiter = threading.Semaphore(value=5)  # Global rate limiting

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

# Configure safe retry strategy
retry_strategy = Retry(
    total=3,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["HEAD", "GET", "OPTIONS"],  # Safe methods only
    backoff_factor=1
)
adapter = HTTPAdapter(max_retries=retry_strategy)
SESSION.mount("https://", adapter)
SESSION.mount("http://", adapter)

def load_payloads(vuln_type):
    """Load payloads with context-aware filtering"""
    try:
        with open(f"payloads/{vuln_type}.json", "r") as f:
            payloads = json.load(f).get("payloads", [])
            return [p for p in payloads if 10 < len(p) < 100]  # Basic sanitization
    except Exception as e:
        logging.warning(f"Payload error: {e}")
        return []

def analyze_response(response_text, vulnerability_type, allow_ai):
    """Secure analysis with opt-in consent"""
    if not allow_ai:
        return False
        
    prompt = f"Analyze this response for {vulnerability_type}. Respond ONLY with 'Yes' or 'No'.\n{response_text[:2000]}"
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=20
        )
        return response.choices[0].message['content'].strip().lower() == 'yes'
    except Exception as e:
        logging.error(f"API error: {e}")
        return False

def test_endpoint(url, method='GET', params=None, data=None):
    """Secure endpoint testing with rate limiting"""
    with rate_limiter:
        time.sleep(0.2)  # Spread requests evenly
        
        try:
            response = SESSION.request(
                method.upper(),
                url,
                params=params,
                data=data,
                headers=HEADERS,
                timeout=(3.05, 15)  # Connect/read timeouts
            )
            
            # Size validation
            if int(response.headers.get('Content-Length', 0)) > 10_000_000:
                logging.warning("Large response detected, skipping analysis")
                return None
                
            return response
        except RequestException as e:
            logging.error(f"Request error: {e}")
            return None

def crawl_website(base_url, max_depth=2):
    """Secure crawler with proper domain validation"""
    visited = set()
    queue = [(base_url, 0)]
    parsed_base = urlparse(base_url)

    while queue:
        url, depth = queue.pop(0)
        if depth > max_depth:
            continue

        try:
            parsed_url = urlparse(url)
            if parsed_url.netloc != parsed_base.netloc:
                continue  # Prevent cross-domain crawling

            response = test_endpoint(url)
            if not response:
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract and validate links
            for link in soup.find_all('a', href=True):
                new_url = urljoin(url, link['href'])
                parsed_new = urlparse(new_url)
                if parsed_new.netloc == parsed_base.netloc and new_url not in visited:
                    visited.add(new_url)
                    queue.append((new_url, depth + 1))

            # Extract forms with CSRF handling
            for form in soup.find_all('form'):
                form_action = urljoin(url, form.get('action', ''))
                if form_action and form_action not in visited:
                    visited.add(form_action)
                    
        except Exception as e:
            logging.debug(f"Crawling error: {e}")
            
    return visited

def verify_false_positive(url, vuln_type):
    """Enhanced verification with alternative payloads"""
    alt_payloads = {
        'directory_traversal': ['....//....//etc/passwd', '%252e%252e%252fetc%252fpasswd'],
        'command_injection': ['%3b+id%3b', '%26%26+whoami']
    }.get(vuln_type, [])
    
    for payload in alt_payloads:
        try:
            test_url = f"{url}?input={payload}" if vuln_type == 'command_injection' \
                      else f"{url}?file={payload}"
            response = test_endpoint(test_url)
            if response and response.status_code in [200, 500]:
                return True
        except Exception as e:
            continue
    return False

def test_directory_traversal(url):
    """Secure path traversal test"""
    payloads = load_payloads("directory_traversal") or [
        '../../etc/passwd', 
        '..%2f..%2f..%2fetc%2fpasswd'
    ]
    
    for payload in payloads:
        response = test_endpoint(url, params={'file': payload})
        if response and ('root:x:' in response.text):
            if verify_false_positive(url, 'directory_traversal'):
                return True
    return False

def test_command_injection(url):
    """Secure command injection test"""
    payloads = load_payloads("command_injection") or [
        ';id',
        '%0awhoami',
        '`id`'
    ]
    
    for payload in payloads:
        response = test_endpoint(url, params={'input': payload})
        if response and ('uid=' in response.text or 'COMMAND_INJECTION' in response.text):
            if verify_false_positive(url, 'command_injection'):
                return True
    return False

def check_tls_ssl(url):
    """Enhanced certificate validation"""
    try:
        parsed_url = urlparse(url)
        hostname = parsed_url.hostname
        context = ssl.create_default_context()
        context.check_hostname = True
        context.verify_mode = ssl.CERT_REQUIRED

        with socket.create_connection((hostname, 443), timeout=10) as sock:
            with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                cert_pem = ssock.getpeercert(binary_form=True)
                cert = x509.load_der_x509_certificate(cert_pem, default_backend())
                
                if cert.not_valid_after.timestamp() < time.time():
                    return "Certificate expired"
                    
                # Check for weak algorithms
                if cert.signature_hash_algorithm.name in ['md5', 'sha1']:
                    return "Weak signature algorithm"
                    
        return None
    except Exception as e:
        return f"TLS error: {str(e)}"

def main():
    parser = argparse.ArgumentParser(description="Secure Penetration Testing Tool")
    parser.add_argument("--target", required=True, help="Target URL")
    parser.add_argument("--crawl", action='store_true', help="Enable website crawling")
    parser.add_argument("--auth", nargs=2, metavar=('USER', 'PASS'), help="Credentials")
    parser.add_argument("--report-format", choices=['json', 'html'], default='json')
    parser.add_argument("--threads", type=int, default=5)
    parser.add_argument("--allow-ai-analysis", action='store_true', 
                       help="Enable AI-assisted analysis")
    args = parser.parse_args()

    # Validate target URL
    if not args.target.startswith(('http://', 'https://')):
        logging.error("Invalid URL scheme")
        exit(1)

    # Handle authentication with CSRF
    if args.auth:
        login_url = urljoin(args.target, '/login')
        login_page = test_endpoint(login_url)
        csrf_token = None
        
        if login_page:
            soup = BeautifulSoup(login_page.text, 'html.parser')
            csrf_token = soup.find('input', {'name': 'csrf_token'})
            csrf_token = csrf_token.get('value') if csrf_token else None
            
        auth_data = {
            'username': args.auth[0],
            'password': args.auth[1],
            'csrf_token': csrf_token or ''
        }
        auth_response = test_endpoint(login_url, method='POST', data=auth_data)
        # ... rest of auth handling

    # Scanning logic with thread-safe reporting
    endpoints = {args.target}
    if args.crawl:
        endpoints.update(crawl_website(args.target))

    with ThreadPoolExecutor(max_workers=args.threads) as executor:
        futures = {executor.submit(scan_endpoint, ep): ep for ep in endpoints}
        for future in as_completed(futures):
            vulns = future.result()
            with report_lock:
                REPORT_DATA['vulnerabilities'].extend(vulns)

    generate_report(args.report_format)

# ... (remaining functions with similar security-focused updates)
